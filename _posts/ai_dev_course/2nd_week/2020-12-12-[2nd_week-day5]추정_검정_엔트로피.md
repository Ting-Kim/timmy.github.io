---
layout: post
title: "[2nd_week-day5]추정,검정,엔트로피"
tags: [ai, probability, distribution]
use_math: true
comments: true
---

## 표본 분포

전수조사는 실질적으로 불가능한 경우가 많으므로 **표본 조사를 통해 모집단에 대한 해석을 진행하는 통계적 추론**을 이용한다.

표본 조사는 반드시 초차가 발생하기 때문에 **적절한 표본 추출 방법**이 필요하다.

(**표본과 모집단과의 관계 이해**가 기반이 되어야 함)

랜덤넘버 생성기 (python)

```python
import random
[random.randint(1,10) for i in range(10)]
```

<br>

## 표본 평균의 분포

표본조사를 통해서 파악하고자 하는 정보는 **모수(parameter)**이다.

모수 - 모평균, 모분산, 모비율 등..

**통계량(statistic)** : 표본의 특성값(표본 평균, 표본 분산 등..)

표본의 평균은 표본의 선택에 따라 달라지고, 그렇기 때문에 **표본평균도 하나의 확률변수**이다.

통계량의 확률분포를 표본분포 (sampling distribution) 라고 한다.

평균이 $\mu$ 이고, 분산이 $\sigma^2$ 인 **정규모집단에서** 표본 n개를 추출하게 되면 그때 발생하는 표본평균의 확률변수 $\bar{X}$는 정규분포를 따른다.

- 표본평균 $\bar{x}={1\over n}\sum_{i=1}^nx_i$
- $\bar{X} \sim N(\mu,{\sigma^2\over n})$

```python
import numpy as np
xbars=[np.mean(np.random.normal(size=10)) for i in range(10000)]
print("mean %f, var %f" %(np.mean(xbars), np.var(xbars)))
```

```python
import numpy as np
xbars=[np.mean(np.random.normal(loc=10, scale=3, size=10)) for i in range(10000)]
print("mean %f, var %f" %(np.mean(xbars), np.var(xbars)))
import matplotlib as plt
h=plt.pyplot.hist(xbars, range=(5,15), bins=30)
```

<br>

### 중심극한정리(central limit theorem)

정규모집단이 아닌 일반 모집단에서 추출된 표본의 측정값으로 부터 추출한 표본평균의 확률변수는 $n \geqq 30$ 일 때 근사적으로 $\bar{X} \sim N(\mu,{\sigma^2\over n})$ 를 만족한다.

<br>

## 모평균의 추정

### 점추정

표본평균이 점 추정값(추정량)이 된다.

<br>

### 구간추정

모평균 $\mu$의 $100(1-\alpha)%$% 신뢰구간 (confidence interval)

\* 신뢰구간 : 100번 중에 x번은 추출한 표본들의 모평균 $\mu$가 포함될 것이라고 보장하는 구간

<br>

- 정규분포에서 $\sigma$ 를 알 떄,

<center>$(\bar x -z_{\alpha/2}{\sigma \over \sqrt{n}},\;\;\;\;\;\; \bar x +z_{\alpha/2}{\sigma \over \sqrt{n}})$</center>

\* ($\mu$의 추정량) $\pm z_{\alpha/2}\times$(추정량의 표준편차)

- **정규분포가 아니거나 표준편차를 모를 때**는 실용적이지 못하다.
- 표본의 크기가 클 때 중심극한 정리를 사용할 수 있다.

<center>$(\bar x -z_{\alpha/2}{s \over \sqrt{n}},\;\;\;\;\;\; \bar x +z_{\alpha/2}{s \over \sqrt{n}})$</center>

(\* $s$=표본표준편차)

```python
import numpy as np
w = [10.7, 11.7, 9.8, 11.4, 10.8, 9.9, 10.1, 8.8, 12.2, 11.0, 11.3, 11.1, 10.3, 10.0, 9.9, 11.1, 11.7, 11.5, 9.1, 10.3, 8.6, 12.1, 10.0, 13.0, 9.2, 9.8, 9.3, 9.4, 9.6, 9.2]
xbar=np.mean(w)
sd=np.std(w, ddof=1)  # 표준편차를 구하는 메서드
											# ddof = 자유도 (delta degree of freedom) : 표본표준편차이기 때문에 전체 자유도에서 1만큼 차이난다.
print("평균 %.2f, 표준편차: %.2f" %(xbar, sd))
import scipy.stats
alpha=0.05
zalpha = scipy.stats.norm.ppf(1-alpha/2) # 1-alpha/2 만큼에 해당하는 고지값을 구해주는 메서드
print("zalpha: ", zalpha)
```

### 모비율의 추정

- n이 충분히 클때 $(n\hat p >5,\;n(1-\hat p) > 5$ 일 때$)$
- $X\sim N(np, np(1-p))$

**확률변수 $X$의 표준화**

- $Z={x-np \over \sqrt{n\hat{p}(1-\hat{p})}}={\hat{p}-p\over \sqrt{\hat{p}(1-\hat{p})\over n}}$
- 근사적으로 표준정규분포 $N(0,1)$을 따른다.

**구간추정**

$z_\alpha$ 의 정의 ⇒ $P(|Z|\leq z_{\alpha/2})=1-\alpha$

<br>

모비율 $p$의 $100(1-\alpha)$% 신뢰구간 (comfidence interval)<br>

⇒$(\hat p-z_{\alpha \over 2}\sqrt{\hat{p}(1-\hat{p})\over n}, \; \hat p+z_{\alpha \over 2}\sqrt{\hat{p}(1-\hat{p})\over n})$

<br>

## 검정

### 통계적가설검정

귀무가설 $H0:\mu -\mu_0$ (귀무가설을 일단 참이라 가정)

대립가설 $H1: \mu > \mu_0$ (새로운 주장)

- 랜덤하게 선택한 표본에서 지금의 $\bar{X}$가 나올 확률을 계산하고, 이 확률이 낮다면 귀무가설이 참이 아니라고 판단한다.
- 낮다는 기준점인 **유의수준 $\alpha$** 도입
- $P(\bar{X} \geq k) \leq \alpha$ 가 되는 $k$를 찾자.
  - $Z={\bar{X}-\mu \over {S/\sqrt n}}\sim N(0,1)$
  - $P(Z \geq z_\alpha) =\alpha$

<br>

**검정단계**

1. $H_0,\;H_1$ 설정
2. 유의수준 $\alpha$ 설정
3. 검정통계량 계산
4. 기각역 또는 임계값 계산
5. 주어진 데이터로부터 유의성 판정

**기각역**

⇒ 대립가설이 무엇인가에 따라서 범위가 달라진다.

귀무가설이 $H_0:\mu=10.5$ 일때,

- 유의수준: $\mu$
- 대립가설에 따라서 귀무가설을 기각할 수 있는 범위
  - $H_1:\mu>10.5$ ⇒ $Z>z_\alpha$
  - $H_1:\mu<10.5$ ⇒ $Z<-z_\alpha$
  - $H_1:\mu \neq 10.5$ ⇒ $\vert Z \vert > z_{\alpha \over 2}$

<br>

**예시)**

- 어떤 농장에서 자신들이 생각하는 계란의 평균 무게가 10.5 g 이라고 홍보하고 있다. 이에 생산된 계란 30개의 표본을 뽑았더니 그 무게가 아래와 같다.
- w=[10.7, 11.7, 9.8, 11.4, 10.8, 9.9, 10.1, 8.8, 12.2, 11.0, 11.3, 11.1, 10.3, 10.0, 9.9, 11.1, 11.7, 11.5, 9.1, 10.3, 8.6, 12.1, 10.0, 13.0, 9.2, 9.8, 9.3, 9.4, 9.6, 9.2]
- 이 농장의 홍보가 맞는지 유의수준 5 % 로 검정하시오.

```python
import numpy as np
w=[10.7, 11.7, 9.8, 11.4, 10.8, 9.9, 10.1, 8.8, 12.2, 11.0, 11.3, 11.1
	, 10.3, 10.0, 9.9, 11.1, 11.7, 11.5, 9.1, 10.3, 8.6, 12.1, 10.0, 13.0
	, 9.2, 9.8, 9.3, 9.4, 9.6, 9.2]
mu = 10.5
xbar=np.mean(w)
sd=np.std(w, ddof=1)
print("평균 %.2f, 표준편차: %.2f" % (xbar, sd))
z=(xbar-mu)/(sd/np.sqrt(len(w)))
print("검정통계량: ", z)
alpha=0.05
import scipy.stats
cri = scipy.stats.norm.ppf(1-alpha/2)
print("임계값: ", cri)
```

<br>

## 교차엔트로피

### 엔트로피

**자기정보(self-information)** : $i(A)$

- $i(A)=log_b({1 \over P(A)})=-log_bP(A)$
  - 자기 정보는 $A_j$를 표현하는 비트수
- 확률이 높은 사건은 정보가 많지 않음.
  - 예) 도들이 들었을 때 개가 짖는 경우보다, 도둑이 들었는데도 개가 안 짖는 경우가 더 많은 정보를 포함하고 있다.
- b는 정보의 단위로 **2(bits)**, e(nats), 10(hartleys) 를 사용함.

<br>

**엔트로피(entropy) 정의 : 자기정보의 평균**

<center>$H(X)=\sum_jP(A_j)i(A_j)=-\sum_jP(A_j)log_2P(A_j)$</center>

<center>$(0 \leq H(X) \leq log_2K)$ </center>

- $K$는 사건의 수인데, 각 사건의 확률이 동일해서 $P(A_j)={1\over K}$ 일 때 $H(X)$가 최댓값을 가진다

엔트로피는 '데이터를 표현할 때 필요한 평균비트수' 이다.

### 교차엔트로피

**교차엔트로피 정의**

⇒두가지 확률분포 상에서 정확한 확률분포에 대해 다른 확률분포가 얼마나 차이나는지..

$H(P,Q)$ : 집합 S상에서 확률분포 P에 대한 확률분포 Q의 교차 엔트로피

- 정확한 확률분포 P를 사용했을 때의 비트수보다 항상 크다.
  - $H(P,Q)=-\sum_{x\in X}P(x)log_2Q(X) \geq -\sum_{x\in X}P(x)log_2P(x)=H(P)$
- 교차엔트로피는 결국 $P$와 $Q$가 얼마나 비슷한지를 나타낸다.
  - 같을 때 ⇒ $H(P,Q)=H(P)$
  - 다를 때 ⇒ $H(P,Q)>H(P)$

**(?) $H(P,Q) <H(P)$ 일 때는?**

<br>

**손실함수 : 학습의 방향을 알려주는 정보를 제공한다.**

**분류 문제에서의 손실함수**

- 기계학습에서는 주어진 대상이 각 그룹에 속할 확률을 제공하는데(예-[0.8, 0.2]), 이 값이 정답인 [1.0, 0.0] 과 얼마나 다른지 측정이 필요하다.
- 원하는 답 $P=[p_1,p_2,\cdot\cdot\cdot,p_n],\; p_1+p_2+\cdot\cdot\cdot+p_n=1$
- 제시된 답 $Q=[q_1,q_2,\cdot\cdot\cdot,q_n],\; q_1+q_2+\cdot\cdot\cdot+q_n=1$ 사이에 얼마나 다른지 척도가 필요하다.
  - 제곱합
    - $\sum(p_i-q_i)^2$
    - 확률이 다를수록 큰 값을 가진다.
    - 하지만 **학습 속도가 느리다.**

<br>

- 교차 엔트로피 $H(P,Q)$
  - 확률이 다를수록 큰 값을 가진다.
  - **학습 속도가 빠르다.**
  - 분류 문제에서 주로 이를 사용한다.

<br>

기계학습의 분류 문제에서 원하는 답은 $P=[p_1,p_2,\cdot\cdot\cdot,p_n]$ 에서 하나만 1이고, 나머지는 0인 경우이다.

- 이 때, 엔트로피 $H(P)=0$ 이다.
- $P$ 중 하나인 $p_k=1.0$ 이라고 하면, $**q_k$의 값이 최대한 커지는 방향으로\*\* 학습을 진행해야 한다.

<br>

교차엔트로피 구하기 실습

```python
import numpy as np
def crossentropy(P,Q):
	return sum([-P[i]*np.log2(Q[i]) for i in range(len(P))])

P=[1,0]
Q=[0.8, 0.2]
print(crossentropy(P,Q))
# 결과 : 0.3219280948873623
Q=[0.5,0.5]
print(crossentropy(P,Q))
# 결과 : 1.0
Q=[0.2, 0.8]
print(crossentropy(P,Q))
# 결과 : 2.321928094887362\
```

<br>

참고 ) 다음주 알차게 보내는 법 소개 (from 이호준 멘토님)

- numpy → 수학적인 내용들 복습 / 구현
- pandas → DataFrame 다루는 방법 → COVID-19 데이터 / 아보카도 데이터 분석 실습할 것
- matplotlib → 시각화 툴(Case Study) + seaborn → 직접 실습

---

- EDA (E) : 탐색적 데이터 분석 → 어떤 데이터를 바탕으로 시각화와 인사이트 뽑아낼 수 있는 것이 목표

**많은 노력이 필요할 것이라는데 무섭구만 🥶. . . .**
